[CharLevelTokenizer] Training tokenizer from 3 files...
[CharLevelTokenizer] Processing "E:/Corpora & Language Resources/모두의 말뭉치/splits\modu-plm-dev.txt": 4230152it [01:56, 36311.46it/s]
[CharLevelTokenizer] Processing "E:/Corpora & Language Resources/모두의 말뭉치/splits\modu-plm-test.txt": 4230152it [02:14, 31508.91it/s]
[CharLevelTokenizer] Processing "E:/Corpora & Language Resources/모두의 말뭉치/splits\modu-plm-train.txt": 73177258it [27:57, 43623.57it/s]
(0) 테러방지법 반대 필리버스터 - 8일 경과.
25 [2, 6871, 5546, 5788, 6523, 5811, 5, 5779, 5318, 5, 7025, 5636, 5805, 6090, 6862, 5, 20, 5, 31, 6386, 5, 4954, 4976, 21, 3]
25 ['[CLS]', '테', '러', '방', '지', '법', '[S]', '반', '대', '[S]', '필', '리', '버', '스', '터', '[S]', '-', '[S]', '8', '일', '[S]', '경', '과', '.', '[SEP]']
[CLS]테러방지법[S]반대[S]필리버스터[S]-[S]8일[S]경과.[SEP]
input_ids: 50 [2, 6871, 5546, 5788, 6523, 5811, 5, 5779, 5318, 5, 7025, 5636, 5805, 6090, 6862, 5, 20, 5, 31, 6386, 5, 4954, 4976, 21, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
masked_input_ids: 50 [2, 6871, 5546, 4, 2722, 5811, 5, 5779, 5318, 5, 7025, 5636, 5805, 6090, 6862, 4, 20, 5, 31, 6386, 5, 4954, 4976, 21, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
token_type_ids: 50 [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]
attention_mask: 50 [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
word_order_ids: 50 [0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 5, 5, 5, 6, 6, 6, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
char_order_ids: 50 [0, 5, 4, 3, 2, 1, 0, 2, 1, 0, 5, 4, 3, 2, 1, 0, 1, 0, 2, 1, 0, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
word_order_ids_masked: 50 [0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
char_order_ids_masked: 50 [0, 5, 4, 3, 2, 1, 0, 2, 1, 0, 7, 6, 5, 4, 3, 2, 1, 0, 2, 1, 0, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
['[CLS]', '테', '러', '[MASK]', '汾', '법', '[S]', '반', '대', '[S]', '필', '리', '버', '스', '터', '[MASK]', '-', '[S]', '8', '일', '[S]', '경', '과', '.', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']


Model: "BOAT-FACTORIZED-CROSS-ORDER"
______________________________________________________________________________________________________________________________________________________
Layer (type)                                     Output Shape                     Param #           Connected to
======================================================================================================================================================
CharLevelInputLayer (InputLayer)                 [(None, 4, 512)]                 0
______________________________________________________________________________________________________________________________________________________
FactorizedOrderedEmbeddingLayer (FactorizedOrder ((None, 512, 128), (None, 1, 1,  600128            CharLevelInputLayer[0][0]
______________________________________________________________________________________________________________________________________________________
TransformerStackedEncoderLayers (TransformerStac ((None, 512, 128), (None, 1, 1,  99584             FactorizedOrderedEmbeddingLayer[0][0]
                                                                                                    FactorizedOrderedEmbeddingLayer[0][1]
______________________________________________________________________________________________________________________________________________________
TransformerPostEncoderDenseLayer (TransformerPos (None, 512, 7703)                1010199           TransformerStackedEncoderLayers[0][0]
                                                                                                    TransformerStackedEncoderLayers[0][1]
======================================================================================================================================================
Total params: 1,709,911
Trainable params: 1,709,911
Non-trainable params: 0
______________________________________________________________________________________________________________________________________________________
ORDER_ENCODING : True
DYNAMIC_STRIP : True
MAX_LENGTH : 512
VOCAB_SIZE : 7703
EMBEDDING_DIM : 128
FACTORIZED_DIM : 64
CROSS_LAYER_SHARING : True
NUM_HEADS : 4
FEEDFORWARD_DIM : 128
NUM_LAYERS : 4
DROPOUT_RATE : 0.1
BATCH_SIZE : 16
LEARNING_RATE : 1e-05
EPOCHS : 1000
SAVED_MODEL_PATH : saved_model


Model: "BOAT"
______________________________________________________________________________________________________________________________________________________
Layer (type)                                     Output Shape                     Param #           Connected to
======================================================================================================================================================
CharLevelInputLayer (InputLayer)                 [(None, 2, 512)]                 0
______________________________________________________________________________________________________________________________________________________
FactorizedEmbeddingLayer (FactorizedEmbeddingLay ((None, 512, 128), (None, 1, 1,  1291584           CharLevelInputLayer[0][0]
______________________________________________________________________________________________________________________________________________________
TransformerStackedEncoderLayers (TransformerStac ((None, 512, 128), (None, 1, 1,  99584             FactorizedEmbeddingLayer[0][0]
                                                                                                    FactorizedEmbeddingLayer[0][1]
______________________________________________________________________________________________________________________________________________________
TransformerPostEncoderDenseLayer (TransformerPos (None, 512, 19023)               2470479           TransformerStackedEncoderLayers[0][0]
                                                                                                    TransformerStackedEncoderLayers[0][1]
======================================================================================================================================================
Total params: 3,861,647
Trainable params: 3,861,647
Non-trainable params: 0
______________________________________________________________________________________________________________________________________________________
DYNAMIC_STRIP : True
MAX_LENGTH : 512
VOCAB_SIZE : 19023
EMBEDDING_DIM : 128
FACTORIZED_DIM : 64
CROSS_LAYER_SHARING : True
NUM_HEADS : 4
FEEDFORWARD_DIM : 128
NUM_LAYERS : 4
DROPOUT_RATE : 0.1
BATCH_SIZE : 16
LEARNING_RATE : 0.0001
EPOCHS : 1000
SAVED_MODEL_PATH : saved_model
